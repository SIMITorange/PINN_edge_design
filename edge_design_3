import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.path import Path
import pandas as pd
import os
from collections import defaultdict

# Set a seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# --- Constants ---
UM_TO_CM = 1e-4
V_PER_UM_TO_MV_PER_CM = 1e-2
L_REF = 10.0
V_REF = 2000.0
EPSILON_R = 9.0
EPSILON_0 = 8.85e-14
RHO_INSIDE = 1e18 * 1.602e-19

# ----------------- PolyMesh Class (with logging) -----------------
class PolyMesh(nn.Module):
    def __init__(self, initial_params):
        super().__init__()
        self.params = nn.ParameterDict()
        for key, value in initial_params.items():
            self.params[key] = nn.Parameter(torch.tensor(float(value), dtype=torch.float32))

    @property
    def device(self):
        return next(self.parameters()).device

    def get_vertices(self):
        device = self.device
        p = self.params
        W_jun = torch.tensor(10.0, device=device)
        verts = [torch.tensor([[0., 1.], [W_jun, 1.], [W_jun, 0.]], device=device)]
        x_current = W_jun
        for i in range(1, 13):
            s_i, w_i = p[f's{i}'], p[f'w{i+1}']
            x_current = x_current + s_i
            verts.append(torch.stack([x_current, torch.tensor(0.0, device=device)]).unsqueeze(0))
            verts.append(torch.stack([x_current, torch.tensor(1.0, device=device)]).unsqueeze(0))
            x_current = x_current + w_i
            verts.append(torch.stack([x_current, torch.tensor(1.0, device=device)]).unsqueeze(0))
            verts.append(torch.stack([x_current, torch.tensor(0.0, device=device)]).unsqueeze(0))
        x_end_plus_5 = x_current + 5.0
        five_t, ten_t = torch.tensor(5.0, device=device), torch.tensor(10.0, device=device)
        p54 = torch.stack([x_end_plus_5 - five_t - p['w13'], five_t])
        p55 = torch.stack([p54[0] - p['w12'] - p['s11'], p54[1] + p['s12']])
        p56 = torch.stack([p55[0] - p['w11'] - p['s10'], p55[1] + p['s11']])
        p57 = torch.stack([p56[0] - p['w10'] - p['s9'], p56[1] + p['s10']])
        p58 = torch.stack([p57[0] - p['w9'] - p['s8'], p57[1] + p['s9']])
        end_verts = torch.stack([
            torch.stack([x_end_plus_5, torch.tensor(0.0, device=device)]),
            torch.stack([x_end_plus_5, torch.tensor(1.0, device=device)]),
            p54, p55, p56, p57, p58,
            torch.stack([W_jun, ten_t]),
            torch.tensor([0.0, 10.0], device=device),
            torch.tensor([0.0, 1.0], device=device)
        ])
        verts.append(end_verts)
        return torch.cat(verts, dim=0)

    def generate_mesh_points(self, n_points_per_region=100, debug_print=False):
        device = self.device
        points = []
        x_prev = torch.tensor(10.0, device=device)
        for i in range(1, 13):
            s_i, w_i = self.params[f's{i}'], self.params[f'w{i+1}']
            rand_s = torch.rand(n_points_per_region, 2, device=device)
            rand_w = torch.rand(n_points_per_region, 2, device=device)
            x_s = x_prev + rand_s[:, 0] * s_i
            y_s = rand_s[:, 1]
            points.append(torch.stack([x_s, y_s], dim=1))
            x_prev_w = x_prev + s_i
            x_w = x_prev_w + rand_w[:, 0] * w_i
            y_w = rand_w[:, 1]
            points.append(torch.stack([x_w, y_w], dim=1))
            x_prev = x_prev + s_i + w_i
        
        all_points = torch.cat(points, dim=0)
        
        if debug_print:
            print(f"[DEBUG] generate_mesh_points: Final points tensor has grad_fn: {all_points.grad_fn}")
        
        return all_points

    def get_boundary_segments(self, vertices):
        lower_mask, upper_mask = vertices[:, 1] < 0.5, ~ (vertices[:, 1] < 0.5)
        if not torch.any(lower_mask): lower_mask[2] = True
        if not torch.any(upper_mask): upper_mask[0] = True
        return vertices[lower_mask], vertices[upper_mask]

    def is_inside(self, points):
        vertices_np = self.get_vertices().detach().cpu().numpy()
        # Add the first vertex to the end to close the polygon for Path
        closed_vertices_np = np.vstack([vertices_np, vertices_np[0]])
        return torch.tensor(Path(closed_vertices_np).contains_points(points.detach().cpu().numpy()), dtype=torch.bool, device=points.device)

# --- PINN Class (with logging) ---
class PINN(nn.Module):
    def __init__(self, hidden_layers=[128, 128, 128], activation=nn.Tanh()):
        super().__init__()
        layers = [nn.Linear(2, hidden_layers[0]), activation]
        for i in range(len(hidden_layers) - 1):
            layers.extend([nn.Linear(hidden_layers[i], hidden_layers[i+1]), activation])
        layers.append(nn.Linear(hidden_layers[-1], 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x / L_REF)

    def dimensional_potential(self, x):
        return self.forward(x) * V_REF

    def compute_laplacian(self, x, debug_print=False):
        if debug_print:
            print(f"[DEBUG] compute_laplacian: Input 'x' has grad_fn: {x.grad_fn}")
            print(f"[DEBUG] compute_laplacian: Input 'x' requires_grad: {x.requires_grad}")

        x.requires_grad_(True)
        u_star = self.forward(x)

        if debug_print:
            print(f"[DEBUG] compute_laplacian: PINN output 'u_star' has grad_fn: {u_star.grad_fn}")

        grad_u = torch.autograd.grad(u_star.sum(), x, create_graph=True)[0]
        lap_x_u = 0
        for i in range(x.shape[1]):
            grad_i = grad_u[:, i:i+1]
            lap_i = torch.autograd.grad(grad_i.sum(), x, create_graph=True, allow_unused=True)[0]
            if lap_i is not None:
                lap_x_u += lap_i[:, i:i+1]
        
        return lap_x_u * (L_REF**2)

    def electric_field(self, x):
        x.requires_grad_(True)
        phi = self.dimensional_potential(x)
        # Using .sum() is essential for autograd to work on a batch of points
        E_v_per_um = -torch.autograd.grad(phi.sum(), x, create_graph=True)[0]
        return torch.norm(E_v_per_um, dim=1, keepdim=True) * V_PER_UM_TO_MV_PER_CM

class FieldTerminationSolver:
    def __init__(self, initial_params, results_dir="results"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        self.poly_mesh = PolyMesh(initial_params).to(self.device)
        self.pinn = PINN().to(self.device)
        
        self.results_dir = results_dir
        if not os.path.exists(self.results_dir): os.makedirs(self.results_dir)

        self.loss_names = [
            'pde', 'boundary', 'field', 'constraint', 'min_constraint', 
            'field_upper_boundary'
        ]
        
        self.log_lambdas = nn.ParameterDict(
            {name: nn.Parameter(torch.tensor(0.0)) for name in self.loss_names}
        ).to(self.device)
        
        self.optimizer = optim.Adam([
            {'params': self.poly_mesh.parameters(), 'lr': 1e-3},
            {'params': self.pinn.parameters(), 'lr': 1e-3},
            {'params': self.log_lambdas.parameters(), 'lr': 1e-3}
        ], amsgrad=True)

        self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.995)
        
        self.target_field = 2.0
        self.min_value = 1.0
        self.poisson_param = ((L_REF * UM_TO_CM)**2 * RHO_INSIDE) / (EPSILON_R * EPSILON_0 * V_REF)
        self.history = defaultdict(list)
        self.initial_losses = None

    def _compute_field_upper_boundary_loss(self, vertices):
        _, upper_b = self.poly_mesh.get_boundary_segments(vertices)
        
        points_to_sample = []
        # Ensure we have at least two vertices to form a segment
        if len(upper_b) < 2:
            return torch.tensor(0.0, device=self.device)

        for i in range(len(upper_b) - 1):
            start_point = upper_b[i]
            end_point = upper_b[i+1]
            for alpha in torch.linspace(0, 1, 10, device=self.device):
                point = start_point + alpha * (end_point - start_point)
                points_to_sample.append(point.unsqueeze(0))
        
        if not points_to_sample:
             return torch.tensor(0.0, device=self.device)

        upper_boundary_points = torch.cat(points_to_sample, dim=0)
        E_mag_upper = self.pinn.electric_field(upper_boundary_points)
        loss = torch.mean(E_mag_upper**2)
        return loss

    def _compute_pde_loss(self, vertices):
        interior_points = self.poly_mesh.generate_mesh_points(n_points_per_region=100)
        x_min, x_max = torch.min(vertices[:,0]).detach(), torch.max(vertices[:,0]).detach()
        y_min, y_max = torch.min(vertices[:,1]).detach(), torch.max(vertices[:,1]).detach()
        n_exterior = interior_points.shape[0]
        exterior_points = torch.rand(n_exterior, 2, device=self.device)
        exterior_points[:, 0] = exterior_points[:, 0] * (x_max - x_min + 20) + x_min - 10
        exterior_points[:, 1] = exterior_points[:, 1] * (y_max - y_min + 10) + y_min - 5
        pde_points = torch.cat([interior_points, exterior_points], dim=0)
        inside_mask = self.poly_mesh.is_inside(pde_points).unsqueeze(1)
        rho_term = torch.where(inside_mask, torch.tensor(self.poisson_param, device=self.device), torch.tensor(0.0, device=self.device))
        laplacian = self.pinn.compute_laplacian(pde_points)
        pde_residual = laplacian + rho_term
        return torch.mean(pde_residual**2)

    def _compute_boundary_loss(self, vertices):
        lower_b, upper_b = self.poly_mesh.get_boundary_segments(vertices)
        u_lower, u_upper = self.pinn(lower_b), self.pinn(upper_b)
        return torch.mean(u_lower**2) + torch.mean((u_upper - 1.0)**2)

    def _compute_field_loss(self, vertices):
        target_indices = [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49]
        valid_indices = [i for i in target_indices if i < len(vertices)]
        if not valid_indices: return torch.tensor(0.0, device=self.device)
        target_points = vertices[valid_indices] + torch.tensor([-0.1, 0.1], device=self.device)
        E_mag = self.pinn.electric_field(target_points)
        return torch.mean((E_mag - self.target_field)**2)

    def _compute_constraint_losses(self):
        p = self.poly_mesh.params
        s_sum = 5.0 + p['s9'] + p['s10'] + p['s11'] + p['s12']
        return (s_sum - 10.0)**2, sum(torch.relu(self.min_value - param)**2 for param in p.values())

    def compute_all_losses(self):
        vertices = self.poly_mesh.get_vertices()
        constraint_loss, min_constraint_loss = self._compute_constraint_losses()
        
        losses = {
            'pde': self._compute_pde_loss(vertices),
            'boundary': self._compute_boundary_loss(vertices),
            'field': self._compute_field_loss(vertices),
            'constraint': constraint_loss,
            'min_constraint': min_constraint_loss,
            'field_upper_boundary': self._compute_field_upper_boundary_loss(vertices),
        }
        return losses
    
    def train(self, epochs=2000, print_every=100):
        print("Computing initial losses for weighting...")
        initial_losses_raw = self.compute_all_losses()
        self.initial_losses = {k: v.item() + 1e-8 for k, v in initial_losses_raw.items()}
        print("Initial losses computed:")
        for name, value in self.initial_losses.items():
            print(f"  - {name}: {value:.4g}")

        for epoch in range(epochs):
            self.optimizer.zero_grad()
            current_losses = self.compute_all_losses()
            
            task_loss = 0
            reg_loss = 0
            for name in self.loss_names:
                normalized_loss = current_losses[name] / self.initial_losses[name]
                weight = torch.exp(-self.log_lambdas[name])
                task_loss += weight * normalized_loss
                reg_loss += self.log_lambdas[name]
            
            total_loss = task_loss + reg_loss
            total_loss.backward()
            
            torch.nn.utils.clip_grad_norm_(self.poly_mesh.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(self.pinn.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(self.log_lambdas.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            if (epoch+1) % 500 == 0: self.scheduler.step()
            
            self._log_history(epoch, current_losses, total_loss, task_loss)
            
            if epoch % print_every == 0 or epoch == epochs - 1:
                self._print_status(epoch, epochs)
                self.visualize_progress(epoch)
        
        print("\n--- Training Complete ---")
        self.visualize_progress(epochs - 1)
        self.plot_parameter_history()
        self.save_history_to_csv()
        # <<< MODIFIED LINE >>>
        # Call the new visualization function after training is complete.
        self.visualize_final_field_distribution()
        print(f"Final results and plots saved in '{self.results_dir}' directory.")

    def _log_history(self, epoch, losses, total_loss, task_loss):
        self.history['epoch'].append(epoch)
        self.history['total_loss'].append(total_loss.item())
        self.history['task_loss'].append(task_loss.item())
        for name, loss_val in losses.items(): self.history[f'{name}_loss'].append(loss_val.item())
        for name, param in self.poly_mesh.params.items(): self.history[name].append(param.item())
        for name, log_lambda in self.log_lambdas.items(): self.history[f'weight_{name}'].append(torch.exp(-log_lambda).item())

    def _print_status(self, epoch, epochs):
        task_loss_val, total_loss_val = self.history['task_loss'][-1], self.history['total_loss'][-1]
        print(f"\n--- Epoch {epoch}/{epochs-1} --- LR: {self.scheduler.get_last_lr()[0]:.1e}")
        print(f"Task Loss: {task_loss_val:.4f} (This should decrease and be positive)")
        print(f"Total Loss (for optimizer): {total_loss_val:.4f}")
        for name in self.loss_names:
            loss_val = self.history[f'{name}_loss'][-1]
            weight_val = self.history[f'weight_{name}'][-1]
            print(f"  - {name.replace('_', ' ').capitalize()} Loss: {loss_val:.4g} (Weight: {weight_val:.4f})")
        p = {k: v.item() for k,v in self.poly_mesh.params.items()}
        s_sum = 5.0 + p['s9'] + p['s10'] + p['s11'] + p['s12']
        print(f"Constraint Check: s9+...+s12+5 = {s_sum:.2f} μm (Target: 10)")

    def visualize_progress(self, epoch):
        plt.ioff(); fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        vertices_np = self.poly_mesh.get_vertices().cpu().detach().numpy()
        closed_vertices_np = np.vstack([vertices_np, vertices_np[0]])
        ax1.plot(closed_vertices_np[:, 0], closed_vertices_np[:, 1], 'ko-', markersize=3); ax1.fill(vertices_np[:, 0], vertices_np[:, 1], 'c', alpha=0.3)
        ax1.set_title(f"Structure at Epoch {epoch}"); ax1.set_xlabel("x (μm)"); ax1.set_ylabel("y (μm)"); ax1.grid(True); ax1.axis('equal')
        ax2.semilogy(self.history['epoch'], self.history['task_loss'], label='Task Loss')
        ax2.set_title("Loss Curve"); ax2.set_xlabel("Epoch"); ax2.set_ylabel("Log Loss"); ax2.grid(True); ax2.legend(); plt.tight_layout()
        plt.savefig(os.path.join(self.results_dir, f"progress_epoch_{epoch}.png")); plt.close(fig)
    
    def plot_parameter_history(self):
        plt.ioff(); s_keys = sorted([k for k in self.poly_mesh.params.keys() if k.startswith('s')]); w_keys = sorted([k for k in self.poly_mesh.params.keys() if k.startswith('w')])
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)
        for key in s_keys: ax1.plot(self.history['epoch'], self.history[key], label=key)
        ax1.set_title('s Parameters vs. Epochs'); ax1.set_ylabel('Value (μm)'); ax1.grid(True); ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        for key in w_keys: ax2.plot(self.history['epoch'], self.history[key], label=key)
        ax2.set_title('w Parameters vs. Epochs'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Value (μm)'); ax2.grid(True); ax2.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        plt.tight_layout(rect=[0, 0, 0.9, 1]); plt.savefig(os.path.join(self.results_dir, "parameter_history.png")); plt.close(fig)

    def save_history_to_csv(self, filename="training_history.csv"):
        history_df = pd.DataFrame(self.history)
        filepath = os.path.join(self.results_dir, filename)
        history_df.to_csv(filepath, index=False)
        print(f"Training history successfully saved to {filepath}")

    # <<< PASTE THIS CORRECTED FUNCTION INTO YOUR FieldTerminationSolver CLASS >>>

    def visualize_final_field_distribution(self, resolution=200):
        """
        Generates and saves a 2D plot of the final electric field distribution.
        """
        print("\nGenerating final electric field distribution plot...")
        plt.ioff()

        # 1. Get final geometry and define the grid for plotting
        final_vertices_np = self.poly_mesh.get_vertices().cpu().detach().numpy()
        # Add a bit of padding around the structure for a better view
        x_min, y_min = final_vertices_np.min(axis=0) - 5
        x_max, y_max = final_vertices_np.max(axis=0) + 5

        x_coords = torch.linspace(x_min, x_max, resolution, device=self.device)
        y_coords = torch.linspace(y_min, y_max, resolution, device=self.device)

        # Create a meshgrid
        xx, yy = torch.meshgrid(x_coords, y_coords, indexing='xy')
        grid_points = torch.stack([xx.flatten(), yy.flatten()], dim=1)

        # 2. Evaluate the electric field on the grid
        self.pinn.eval() # Set model to evaluation mode (good practice)

        # --- BUG FIX ---
        # The `with torch.no_grad()` block has been REMOVED from here.
        # We need gradients enabled so that `pinn.electric_field`, which uses
        # torch.autograd.grad internally, can function correctly.
        E_mag = self.pinn.electric_field(grid_points)

        # Reshape the E-field magnitude back to the grid shape and move to CPU for plotting
        E_mag_grid = E_mag.reshape(resolution, resolution).cpu().detach().numpy()

        # 3. Create the plot
        fig, ax = plt.subplots(figsize=(10, 8))

        # Use pcolormesh for a smooth heatmap
        # We clip the max value for better color contrast, as some points might have extreme field spikes.
        vmax = self.target_field * 1.5
        im = ax.pcolormesh(xx.cpu().numpy(), yy.cpu().numpy(), E_mag_grid, 
                           cmap='jet', shading='gouraud', vmin=0, vmax=vmax)

        # Add a color bar
        cbar = fig.colorbar(im, ax=ax)
        cbar.set_label('Electric Field Magnitude (MV/cm)')

        # Overlay the final geometry
        closed_vertices_np = np.vstack([final_vertices_np, final_vertices_np[0]])
        ax.plot(closed_vertices_np[:, 0], closed_vertices_np[:, 1], 'k-', linewidth=1.5, label='Final Geometry')
        ax.fill(final_vertices_np[:, 0], final_vertices_np[:, 1], color='white', alpha=0.3)

        # Final plot adjustments
        ax.set_title('Final Electric Field Distribution')
        ax.set_xlabel('x (μm)')
        ax.set_ylabel('y (μm)')
        ax.axis('equal')
        ax.grid(True, linestyle='--', alpha=0.5)
        ax.legend()
        plt.tight_layout()
    
        # Save the figure
        filepath = os.path.join(self.results_dir, "final_electric_field.png")
        plt.savefig(filepath, dpi=300)
        plt.close(fig)

        print(f"Final electric field plot saved to {filepath}")
        self.pinn.train() # Set model back to training mode just in case

if __name__ == "__main__":
    initial_params = {
        's1':1.5, 's2':1.5, 's3':1.5, 's4':1.5, 's5':1.5, 's6':1.5,
        's7':1.5, 's8':1.5, 's9':1.5, 's10':1.5, 's11':1.5, 's12':1.5,
        'w2':1.5, 'w3':1.5, 'w4':1.5, 'w5':1.5, 'w6':1.5, 'w7':1.5,
        'w8':1.5, 'w9':1.5, 'w10':1.5, 'w11':1.5, 'w12':1.5, 'w13':1.5
    }
    
    solver = FieldTerminationSolver(initial_params)
    # Reducing epochs for a quick test run, you can set it back to 5000
    solver.train(epochs=6000, print_every=100)
